{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General intuition\n",
    "For this tutorial, we're going to be guessing which author wrote a string of text based on the normalized unigram frequency. That's just a fancy way of saying that we're going to count how often each author uses every word in our training data and then divide by the number of total words they wrote. Then, if our test sentence has words that we've seen one author use a lot more than the others, we will guess that that person is probably the author.\n",
    "\n",
    "Let's imagine that this is our training corpus:\n",
    "\n",
    "Author one: \"A very spooky thing happened. The thing was so spooky I screamed.\"\n",
    "Author two: \"I ate a tasty candy apple. It was delicious\"\n",
    "And that this is our test sentence that we want to figure out who wrote:\n",
    "\n",
    "Author ???: \"What a spooky thing!\"\n",
    "Just looking at it, it seems more likely that author one wrote this sentence. Author ones says both \"spooky\" and \"thing\" a lot, while author two does not (at least, based on our training data). Since we see both \"spooky\" and \"thing\" in our test sentence, it seems more likely that it was written by author one than author two--even though the test sentence does have the word \"a\" in it, which we have seen author two use too.\n",
    "\n",
    "In the rest of this tutorial we're going to figure out how to translate this intution into code.\n",
    "\n",
    "Read in some helpful NLP libraries & our dataset\n",
    "For this tutorial, I'm going to be using the Natural Language Toolkit, also called the \"NLTK\". It's an open-source Python library for analyzing language data. The really nice thing about the NLTK is that it has a really helpful book that goes step-by-step through a lot of the common NLP tasks. Even better: you can get the book for free here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "texts = pd.read_csv(\"./spooky_author/train.csv\")\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.groupby.DataFrameGroupBy object at 0x000002513579B198>\n"
     ]
    }
   ],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "print(byAuthor)\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "    \n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at how often each writer uses specific words. Since this is a Halloween competition, how about \"blood\", \"scream\" and \"fear\"? üëªüò®üßõ‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood: EAP\n",
      "0.00014646397201676582\n",
      "blood: HPL\n",
      "0.00022992337803427008\n",
      "blood: MWS\n",
      "0.00022773011333545174\n",
      "\n",
      "scream: EAP\n",
      "1.7231055531384214e-05\n",
      "scream: HPL\n",
      "9.196935121370803e-05\n",
      "scream: MWS\n",
      "2.6480245736680435e-05\n",
      "\n",
      "fear: EAP\n",
      "0.00010338633318830528\n",
      "fear: HPL\n",
      "0.0005748084450856752\n",
      "fear: MWS\n",
      "0.0006196377502383222\n"
     ]
    }
   ],
   "source": [
    "# see how often each author says \"blood\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"blood: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('blood'))\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"scream\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"scream: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('scream'))\n",
    "    \n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"fear\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"fear: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('fear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this general principle to guess who might have been more likely to write the sentence \"It was a dark and stormy night.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPL'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to guess authorship is to use the joint probabilty that each \n",
    "# author used each word in a given sentence.\n",
    "\n",
    "# first, let's start with a test sentence\n",
    "testSentence = \"It was a dark and stormy night.\"\n",
    "\n",
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empy dataframe to put our output in\n",
    "testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence...\n",
    "    for j  in preProcessedTestSentence:\n",
    "        # find out how frequently the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "        testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "        \n",
    "# empty dataframe for the probability that each author wrote the sentence\n",
    "testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "# and our winner is...\n",
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                      HPL\n",
       "jointProbability    2.48359e-20\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
